#    "OBR_Noise_reducer" (v1.0)
#    Copyright 2016 Soren Heinze
#    soerenheinze <at> gmx <dot> de
#    5B1C 1897 560A EF50 F1EB 2579 2297 FAE4 D9B5 2A35
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU General Public License as published by
#    the Free Software Foundation, either version 3 of the License, or
#    (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU General Public License for more details.
#
#    You should have received a copy of the GNU General Public License
#    along with this program.  If not, see <http://www.gnu.org/licenses/>.


# A small program to automatically detect outliers in analyzed OBR data.
# 
# This program was written for files generated by the OBR-measurement/analyzing
# program, as stated in the accompanying manual to this program, in connection
# with the OBR_analyzer_simple.py program.
# 
# It is assumed that just ONE property if interest (e.g. either strain OR 
# temperature) is saved in each file.
# In this case the left column should contain the positional values and 
# the right column the values for the property of interest.
# 
# Before the table with the actual values some other data is present in the 
# analysis-file.
# However the table starts usually with some keywords directly followed
# by the data.
# In the functions below I use the following keywords: "Length", "microstrain", 
# "(deg C)", "Spectral". This should cover the most common cases. If this is not
# the case, extra keywords can simply be added in these functions.
# 
# It is also assumed that neither the central position of the area of interest, 
# or other (virtual) strain gauge parameters changed during the analysis of 
# the OBR data.
# In general (and especially ) this should be the case.
# 
# In addition the following assumptions are made regarding the data
# 0. the data is mainly "good" and has some outliers
# 1. the data is sectionwise "flat"
# 2. three (or more) "flat points" are considered to be one section
# 3. linear relation between flat sections
# 4. equally spaced points in x-direction
# 5. I actually need TWO flat sections to be able to "interpolate"
# 
# Due to rounding errors of the OBR analysis program, assumption 4 is not 
# entirely true. But the error in position is far below the measurement
# resolution and this program compensates for such eventualities.
# 
# Since some outliers escape this algorithm, this program creates (initially) 
# identical noise reduced files in two locations. So that the user can 
# manually correct the remaining outliers.
# See also the accompanying manual to this program.
# 
# For higher user friendlyness to detect the remaining outliers, each outlier 
# corrected dataset is automatically plotted and saved as a PNG file.
# This requires the installation of matplotlib.
# See the relevant document regarding this issue.
# 
# ATTENTION: It is assumed that the OBR_Filesorter.py program was used to sort 
# the analysis files. This includes the creation of certain folders that are 
# assumed to exist throughout this program. 
# If not these folders need to be created. 
# See also the accompanying manual to this program.
# 
# The parameters: maximum_difference_between_points (to determine how flat a 
# "flat section" is (see also point 1 above) and 
# maximum_difference_for_interpolation (as maximum allowed deviation from the 
# linear line between two flat sections to still count as datapoint, see also
# point 3 above) worked out fine for me, but probably need to be adjusted.
# 
# The main part of this file is the definition of functions. The actual 
# execution of these functions (a.k.a. running the program) takes place at the 
# end of this file.
# 
# ATTENTION: It is assumed that the running reference approach was used to 
# analyze the OBR rawdata.
# This means that from measurement two onwards, each file was analyzed
# once with the last measurement as reference and once with the second to last
# measurement as reference file. The filenames of the latter analysis should 
# contain "_older_reference" to indicate this.
# This is the case of the OBR_analyzer_simple.py program was used for the 
# analysis of the OBR rawdata.

# On older computers it may take a while before all modules are  loaded.
# Impatient users may think the program crashed. Hence, this message.
print "Loading modules (this may take some seconds) ..."

from copy import deepcopy
import matplotlib.pyplot as plt
import os
# shutil is here for using the correct operative specific move commands.
import shutil


# This function reads the data from a file and returns a list that
# contains tuples which contain the data in the correct order.
# The correct order is important to find the "flat" sections.
def data_from_one_file(infile):
	# This list will be returned and contains just the positional data
	# and the values of the property of interest as tuples.
	data = []
	# Before the actual table with the data some metadata is written
	# into the files.
	# Right before the data a line with a keyword (e.g. "Length") appears.
	# This will sett data_is_here to True. As long as this is not the 
	# case, nothin will be written into the data-list defined above.
	data_is_here = False
	# Open the file with the data ...
	with open(infile, 'r') as f:
		# ... scan through it line for line ...
		for line in f:
			# ... and if data_is_here is set to True ...
			if data_is_here:
				# ... split this line at the "\t" (raw will be a list
				# that contains the position as first value and the 
				# value of the property of interest as second value but
				# both will be strings and NOT numbers), ...
				raw = line.strip().split('\t')
				# ... convert the strings to numbers, ...
				a = float(raw[0])
				b = float(raw[1])
				# ... create a tuple with these numbers ...
				one_coord = (a, b)
				# ... and append them to data; ...
				data.append(one_coord)
			# ... but if data_is_here is still False, check if one of the 
			# keywords appears, and if this is the case ...
			elif 'Length' in line or 'microstrain' in line \
						or '(deg C)' in line or 'Spectral' in line:
				# ... set data_is_here to True.
				data_is_here = True

	# When the end of the file is reached, return the data.
	return data



# This function reads the data from the previous measurement and the 
# measurement with an older reference. It returns a dict (!).
# I don't need the correct order later, but the ability to access
# the y-values to given x-values directly.
# 
# This is very much like data_from_one_file() with some exceptions
# I mention below.
def get_extra_data(other_reference_measurement, previous_measurement):
	# In contrast to data_from_one_file() I create here dictionaries and
	# NOT lists.
	other_reference = {}
	previous = {}

	data_is_here = False
	# First, get the data from the file with the second to last measurement
	# as reference.
	with open(other_reference_measurement, 'r') as f:
		for line in f:
			if data_is_here:
				raw = line.strip().split('\t')
				a = float(raw[0])
				b = float(raw[1])
				# Here is another diference to data_from_one_file(). Since
				# 
				other_reference[a] = b
			elif 'Length' in line or 'microstrain' in line \
						or '(deg C)' in line or 'Spectral' in line:
				data_is_here = True

	# Don't forget to set data_is_here back to False because otherwise all the 
	# metadata will be read and that will lead to errors down the line.
	data_is_here = False

	# Second, get the data from the previous measurement.
	with open(previous_measurement, 'r') as f:
		for line in f:
			if data_is_here:
				raw = line.strip().split('\t')
				a = float(raw[0])
				b = float(raw[1])
				previous[a] = b
			elif 'Length' in line or 'microstrain' in line \
						or '(deg C)' in line or 'Spectral' in line:
				data_is_here = True

	return (other_reference, previous)



# This function can checks if the two points to the left/right to points 
# 
# data is the list that contains the datapoints as tuples.
# which_point is the position of the datapoint in question in this list;
# NOT the "x"-value!
# ATTENTION: which_point must be >= 2!
# ATTENTION 2: remember that counting starts at ZERO!
# ATTENTION 3: The first two points are NOT to be evaluated.
# See also find_flat_sections().d
# I could consider those points too, but that would require some
# if-statements I deem not necessary.
def check_to_the_side(data, which_point, maximum_difference_between_points, \
												which_direction = 'right'):
	# one_step is used to check one (or two) step(s) to the left/right.
	# In case I need to check to the right one_step needs to be positive ...
	one_step = 1
	# ... in case I need to check to the left, one_step needs to be negative.
	if which_direction == 'left':
		one_step = -1

	# Don't forget, that data is a list, that contains tuples.
	value = data[which_point][1]
	first_side_value = data[(which_point + one_step)][1]
	second_side_value = data[(which_point + 2*one_step)][1]

	# The values have to be in "one line" and the difference
	# between them shall not be greater than a given value.
	difference_point_to_first = abs(value - first_side_value)
	difference_first_to_second = abs(first_side_value - second_side_value)

	# Just if all three points are in one line (with the given allowed
	# difference between two points), this point is considered to be
	# in a straight section.
	if difference_point_to_first <= maximum_difference_between_points and \
			difference_first_to_second <= maximum_difference_between_points:
		return True
	else:
		return False



# This is more or less the same as check_to_the_side() but here I check the 
# point to left and the point to the right.
def check_middle(data, which_point, maximum_difference_between_points):
	value = data[which_point][1]
	left_side_value = data[(which_point - 1)][1]
	right_side_value = data[(which_point + 1)][1]

	difference_to_left = abs(value - left_side_value)
	difference_to_right = abs(value - right_side_value)

	if difference_to_left <= maximum_difference_between_points and \
			difference_to_right <= maximum_difference_between_points:
		return True
	else:
		return False



# This is just a summary of the check_to_the_side() and check_middle()
# to have just one line in later sections of the program.
def belongs_to_straight_section(data, which_point, maximum_difference_between_points):
	if not check_to_the_side(data, which_point, maximum_difference_between_points, \
				'right') and not check_to_the_side(data, which_point, \
				maximum_difference_between_points, 'left') and not \
				check_middle(data, which_point, maximum_difference_between_points):
		return False
	else:
		return True



# I need to transfer quite often data into dicts throughout this program.
# Thus I created just function for that to keep the functions where this 
# is done more orderly.
# point must be the tupel (x_value, y_value).
def transfer_point_into_dict(the_dict, point):
	x_value = point[0]
	y_value = point[1]
	the_dict[x_value] = y_value



# This functions figures out if which of the datapoints in data are within a 
# flat section (three points in one line, without outliers in between).
# 
# This function combines belongs_to_straight_section() and 
# transfer_point_into_dict() to actually find the flat sections in the data 
# and transfer it to the "these points are ok"-data.
def find_flat_sections(data, maximum_difference_between_points):
	# Create the containers for the real data and the outliers.
	# After this function is finished, real_data will contain so far just
	# the points in flat sections.
	# outliers contains all the remaining points. 
	# If outliers turn out to be real data at a later step, these will be 
	# transfered into real_data and deleted from outliers.
	real_data = {}
	outliers = {}
	# For EACH of the datapoints it will be checked if it belongs to
	# a flat section. If yes, it will be transfered into real_data, 
	# if not, into outliers.
	#
	# ATTENTION: DON'T EVALUATE THE FIRST TWO AND THE LAST TWO POINTS!
	# THE FUNCTIONS ABOVE CAN'T DO THAT!
	# With the data I have, this doesn't matter, since I usually have 
	# a lot of unnecessary data at the sides.
	for i in range(2, (len(data) - 2)):
		# If a point belongs to a flat section ...
		if belongs_to_straight_section(data, i, maximum_difference_between_points):
			# ... transfer it to real_data ...
			transfer_point_into_dict(real_data, data[i])
		# ... otherwise, ...
		else:
			# ... transfer it ti outliers.
			transfer_point_into_dict(outliers, data[i])

	return (real_data, outliers)



# To calculate the distance between points in x-direction.
# I need this quite often so I wrote an extra function for that.
# real_data is a dict.
# ATTENTION: This function assumes that the points are equally spaced 
# in x-direction. Hence it just 
def calculate_constant_x_distance(real_data):
	x_values = sorted(real_data.keys())
	# Not all points are equally spaced in x-direction. See also the comment 
	# above and in find_start_and_end_of_unknown_sections().
	#  So I figure first out all possible distances between two adjecent points
	#  and put them into a set and into a list.
	# A set contains just unique values. So if a value shall be added to 
	# the set that already is in the set, it will not be added again.
	# However, in a list two identical values can be in there.
	# Afterwards, I check for each element in the set how often it appears
	# in the list and return the most common value.	
	set_of_all_distances = set()
	all_distances = []

	# First figure out all distances between the points and add them 
	# to the list and set ...
	for i in range(len(x_values) - 1):
		this_distance = x_values[i + 1] - x_values[i]
		set_of_all_distances.add(this_distance)
		all_distances.append(this_distance)

	# ... then find the distance that is most common.
	so_often = 0
	constant_x_distance = None
	for distance in set_of_all_distances:
		so_often_new = all_distances.count(distance)
		if so_often_new > so_often:
			so_often = so_often_new
			constant_x_distance = distance

	return constant_x_distance



# This function checks the distance between all the points in the 
# flat sections. If the distance between two points is bigger then the most 
# common distance, these two points are the start- and end-point of the
# unknown section.
# See also the comment below.
def find_start_and_end_of_unknown_sections(real_data):
	start_end_points = []
	# It is assumed that there actually are at least two flat sections.
	if len(real_data) == 0:
		# If this is not the case, the data is just noise.
		print "The data is just noise"
	else:
		x_values = sorted(real_data.keys())
		# All points have (in theory) the same (characteristic) distance from 
		# each other in x-direction (position or length). That means that a gap
		# between two flat sections due to an outlier is equivalent
		# to two points NOT having this distance (in x-direction) to each 
		# other. Since I look just at the already determined flat section
		# data here, I should easily figure out these gaps by checking if 
		# two points have a different distance from each other than this
		# characteristic distance.
		# However, as mentioned above sometimes the positional value is 
		# NOT equidistant to the previous or following point due to rounding 
		# errors of the OBR measurement/analysis program. The error is very 
		# small and may be positive or negative. 
		# So simply checking if the distance between two points is not 
		# the characteristic distance may "find" gaps where there actually are
		# none.
		# BUT a gap contains at minimum width one datapoint. So two separate flat
		# sections are (in theory) at least two characteristic distances apart. 
		# That means checking if two points are more then 1 1/2 characteristic 
		# distances apart from each other will detect just real gaps between 
		# the flat sections.
		# 
		# This is an issue that comes up at several places throughout 
		# this whole program.
		minimum_distance_for_gap = calculate_constant_x_distance(real_data)
		minimum_distance_for_gap += minimum_distance_for_gap / 2

		# The last flat section can NOT have a "continuation" to the
		# "next" flat section (since it already is the last).
		# Hence it doesn't matter that I don't check the very last value.
		for i in range((len(x_values) - 1)):
			x_distance = abs(x_values[i] - x_values[i + 1])
			# Just if the distance between two points is significantly bigger 
			# then the most common distance between points it shall be 
			# recognized as a gap.
			if x_distance > minimum_distance_for_gap:
				start = (x_values[i], real_data[x_values[i]])
				end = (x_values[i + 1], real_data[x_values[i + 1]])
				start_end_points.append((start, end))

	return start_end_points



# This is to find the slope between two given points.
# intervalls is a list that contains tuples. Each tuple containins two tuples, 
# that contain the x- and y-coordinates.
# See find_start_and_end_of_unknown_sections()
def find_slopes_of_undefined_sections(intervalls):
	all_section_data = []
	# If there are no gaps in the data, I can not interpolate.
	# Most often this is because the data is just very good and does not 
	# have outliers.
	if len(intervalls) == 0:
		print "No intervalls to interpolate between. No worries, probably it is just good data :) ."
	else:
		# For each gap ...
		for intervall in intervalls:
			# ... simply calculate the linear slope between two points.
			y_difference = intervall[1][1] - intervall[0][1]
			x_difference = intervall[1][0] - intervall[0][0]
			slope = y_difference / x_difference

			# In the end add the slope value to the start-/end-point tuples.
			# Since one can not append or add to tuples, I need to create
			# whole new tuples.
			all_section_data.append((intervall[0], intervall[1], slope))
	
	return all_section_data



# This is just a summary of the find_start_and_end_of_unknown_sections() and
# find_slopes_of_undefined_sections() to have just one line in reduce_noise().
# Since this will run after the initial check for "flat"-sections it takes
# this data as input.
def find_undefined_sections(first_run_real_data):
	# First find the start- and end-point of the gaps between the flat 
	# sections ...
	undefined_sections = find_start_and_end_of_unknown_sections(first_run_real_data)
	# ... then calculate the slope of the line between these two points.
	undefined_sections_with_slopes = find_slopes_of_undefined_sections(undefined_sections)

	return undefined_sections_with_slopes



# To know how many points there are in the intervall between two flat sections.
# startpoint and endpoint are tuples.
def so_many_points_in_intervall(startpoint, endpoint, constant_x_distance):
	# I actually calculate NOT the number of points but how many "jumps"
	# of constant_x_distance are there between startpoint and endpoint.
	# The number of "jumps" must of course be reduced by one to get the
	# number of points.
	# 
	# Since int() returns everything before the comma, this is 
	# unaffected by the above mentioned rounding errors in the x-values
	# of the points.
	return int((endpoint[0] - startpoint[0])/constant_x_distance - 1)



# To calculate where the point should be if there is a linear 
# slope between two flat sections.
def calculate_point(start, constant_x_distance, slope, which_point):
	point_x = start[0] + constant_x_distance*(which_point + 1)
	# This is how the y-value should be.
	point_y = slope*(constant_x_distance*(which_point + 1)) + start[1]

	return (point_x, point_y)



# Calculations sometimes come up with values that are not exactly as
# measured, in this case the nearest value is most likely the one I'm
# looking for and shall be used.
# find_nearest() searches the whole dict for the value I'm looking for.
def find_nearest(dictionary, x_value_to_be_found):
	x_values = dictionary.keys()
	old_difference = 10000000000000000000000.0
	nearest_value = -50000000000000000000000.0

	# The keys in the dicts I use in this program are the x-values of the 
	# points.
	# This loop calculates for all points the difference to the x-value
	# I'm looking for but "remembers" just the smallest difference and the
	# x-value it belongs to. This will be returned once all values are checked.
	for x_value in x_values:
		difference = abs(x_value - x_value_to_be_found)
		if difference < old_difference:
			old_difference = difference
			nearest_value = x_value

	return nearest_value



# This compares the calculated y-value with the measured y-value
# and returns True if the difference is <= maximum_difference_for_interpolation.
# 
# Since the many times mentioned rounding errors of the x-values, it also 
# returns the x_value that shall be used when saving this point.
def point_is_real(outliers, calculated_point, maximum_difference_for_interpolation):
	# calculated_point contains the "perfect" x-values if all points have
	# absolutely the same distance.
	# Again due to the rounding errors, this may not be the real x-value
	# and thus will not be found in the outliers-dict. If this is the case ...
	try:
		x_value = calculated_point[0]
		real_point_y = outliers[x_value]
	# ... find the point that is nearest to this "perfect" calculated position.
	# I can do it this way, since the rounding errors are small and the actual
	# point is always the nearest to this calculated "perfect" position.
	except KeyError:
		x_value = find_nearest(outliers, calculated_point[0])
		real_point_y = outliers[x_value]

	# Calculate the difference between tha actual point and the expected
	# value if this point would have been on the line between flat sections.
	difference = abs(real_point_y - calculated_point[1])

	# If this difference is small enough ...
	if difference <= maximum_difference_for_interpolation:
		# ... return True and what the (actual) x-value of this point is.
		return (True, x_value)
	else:
		return (False, None)



# This function returns JUST the x-values of the points that turn out
# to be NOT outliers!
def find_more_points_in_one_intervall(intervall_data, constant_x_distance, outliers, \
													maximum_difference_for_interpolation):
	# A list that contains all x-values of the points that are figured out as 
	# real values.
	real_points = []
	start = intervall_data[0]
	end = intervall_data[1]
	slope = intervall_data[2]

	# Calculate how many point there actually need to be in an intervall.
	points_in_intervall = so_many_points_in_intervall(start, end, constant_x_distance)
	for i in range(points_in_intervall):
		# Then for each of these assumed point calculate the x-values they 
		# would have if the point would be exactly on the line.
		assumed_point = calculate_point(start, constant_x_distance, slope, i)
		# Afterwards check if the actual point is in the vicinity of this
		# calculated point.
		# real_point is a tuple that contains True if the point is real and
		# the x-value of this point.
		real_point = point_is_real(outliers, assumed_point, \
											maximum_difference_for_interpolation)
		# If the point in question is NOT an outlier ...
		if real_point[0]:
			# ... append its x-value to the list ...
			real_points.append(real_point[1])

	# ... and in the end return the list with all the x-values of the points 
	# that turned out not to be outliers.
	return real_points



# To check if real data points were dismissed initially because these
# were not in a "flat"-section.
# See also comment in reduce_noise().
# Returned are the (probably expanded) real data dict and the 
# (probably reduced) outlier dict.
def check_if_point_is_on_line(undefined_sections, first_run_real_data, \
					first_run_outliers, maximum_difference_for_interpolation):
	# This is probably not necessary but I don't like to mess with results
	# I already have, so I prefer to make a real copy.
	# more_real_data will be first_run_real_data with more points added to 
	# it; less_outliers will be first_run_outliers with points taken out of it.
	# Hence the names.
	more_real_data = deepcopy(first_run_real_data)
	less_outliers = deepcopy(first_run_outliers)

	# If there were no flat sections, the data is just noise.
	if len(more_real_data) == 0:
		print "The data is just noise"
	else:
		# This is just to figure out (again) the distance in x-direction.
		constant_x_distance = calculate_constant_x_distance(more_real_data)
		# Now the real thing starts.
		for intervall in undefined_sections:
			# find_more_points_in_one_intervall() figures out points in the 
			# intervall (gap) that actually are NOT outliers and returns
			# the x-values of these points.
			# more_points are just the x-values!
			more_points = find_more_points_in_one_intervall(intervall, \
									constant_x_distance, less_outliers, \
									maximum_difference_for_interpolation)

			# Then find the y-values to these x-values ...
			for x_value in more_points:
				point = (x_value, less_outliers[x_value])
				# ... add the (now as real recognized) points to the
				# real data dict ... 
				transfer_point_into_dict(more_real_data, point)
				# ... and at the end delete the point from the outliers.
				del less_outliers[x_value]

	return (more_real_data, less_outliers)



# Sometimes outliers actually are peaks that show up consistently.
# If this is the case a check of the same data, obtained in a different way
# may reveal these peaks.
# This function does that.
# 
# other_reference_data is the same measurement but evaluated with a 
# reference two measurements back (and not just one)
# previous_data is the measurement from before evaluated in the regular way.
# If the supposed outlier is real it's value, added to previous_data should 
# add upp to other_reference_data.
# 
# ATTENTION: It is possible that noise from all the data compared in this 
# function adds up so that an outlier is actually seen as real data.
# In a testcase this happened approx. 5 times in 50.000 datapoints.
# But these points are "classical" outliers and will be detected in the 
# very last step.
def check_other_data_for_peaks(second_run_real_data, second_run_outliers, \
			other_reference_data, previous_data, maximum_difference_between_points):
	more_real_data = deepcopy(second_run_real_data)
	less_outliers = deepcopy(second_run_outliers)

	# .keys() returns just the x-values of the points.
	for outlier_x in less_outliers.keys():
		# The same issue that appears due to rounding errors. 
		# See also e.g. comment in point_is_real() since there the same 
		# try ... except is implemented due to the same reason.
		try:
			calculated = less_outliers[outlier_x] + previous_data[outlier_x]
		except KeyError:
			# I don't need to "correct" for the values in less_outliers since
			# I take the value from there. 
			correct_x_value = find_nearest(previous_data, outlier_x)
			calculated = less_outliers[outlier_x] + previous_data[correct_x_value]

		try:
			difference = abs(calculated - other_reference_data[outlier_x])
		except KeyError:
			correct_x_value = find_nearest(other_reference_data, outlier_x)
			difference = abs(calculated - other_reference_data[correct_x_value])

		if difference <= maximum_difference_between_points:
			real_point = (outlier_x, less_outliers[outlier_x])
			transfer_point_into_dict(more_real_data, real_point)
			del less_outliers[outlier_x]

	return (more_real_data, less_outliers)



# For adding up the data I need to fill in values. 
# Here it is assumed that all remaining outliers actually are outliers, thus 
# I don't delete any longer from the outlier dict.
def interpolate_values(undefined_sections, last_run_real_data):
	even_more_real_data = deepcopy(last_run_real_data)
	constant_x_distance = calculate_constant_x_distance(even_more_real_data)

	for intervall_data in undefined_sections:
		start = intervall_data[0]
		end = intervall_data[1]
		slope = intervall_data[2]

		# Calculate how many point there actually need to be in an intervall.
		points_in_intervall = so_many_points_in_intervall(start, end, \
															constant_x_distance)
		for i in range(points_in_intervall):
			# Then calculate for each of these points the value it should have
			# on the slope.
			assumed_point = calculate_point(start, constant_x_distance, slope, i)

			# Find the nearest point to that calculated point.
			nearest_point = find_nearest(even_more_real_data, assumed_point[0])
			difference = abs(assumed_point[0] - nearest_point)
			# This method interpolates all points between two flat sections.
			# But in the steps before I already added points in such gaps
			# if these turned out not to be outliers.
			# 
			# As mentioned already several times, do rounding errors occur
			# in the x-values. So the constant_x_distance is not as constant 
			# in real data as I assumed. 
			# 
			# Hence if I just search for assumed_point[0] to be in 
			# even_more_real_data I may add points which are already in there 
			# but which are a bit longer/lesser away from the last point than 
			# constant_x_distance. 
			# 
			# However, IF the point is already in even_more_real_data (so if it 
			# was recognized as a real point in a previous step), its x-value
			# is very near assumed_point[0] since these rounding errors are 
			# small. It is in any case nearer then constant_x_distance/2.
			# 
			# Thus if the nearest point to the assumed_point is farther away 
			# then this distance, assumed_point must be added to the real
			# data since there is no point at this position, yet.
			if difference >= constant_x_distance/2:
				transfer_point_into_dict(even_more_real_data, assumed_point)

	return even_more_real_data



# There can be data before the first or after the last flat section. 
# This may be noise, or real data with a steep slope (thus not "flat"). 
# I can NOT interpolate in this case. Thus I just fill up with the 
# original value.
# original_data is a list, last_run_real_data is a dict.
def fill_up_the_data(original_data, last_run_real_data):
	final_real_data = deepcopy(last_run_real_data)
	constant_x_distance = calculate_constant_x_distance(last_run_real_data)

	# Yes, here I go through the complete original data.
	for x, y in original_data:
		if x not in final_real_data:
			nearest_x = find_nearest(last_run_real_data, x)
			# See comment in interpolate_values why I use 
			# constant_x_distance/2 here.
			if abs(nearest_x - x) >= constant_x_distance/2:
				final_real_data[x] = y

	return final_real_data



# To write the noise reduced data into a file.
# Since here the order is important again, final_data needs to be a list, 
# that contains the tuples with coordinates.
def write_to_file(outfile, final_data):
	with open(outfile, 'w') as f:
		for x, y in final_data:
			f.write('%s\t%s\n' % (x, y))



# This function makes pictures of each data-set that shall help me with 
# manually figuring out the last remaining outliers that are not detected
# by this program.
def plot_data(original_data, reduced_noise_data, save_file):
	with_noise = []
	without_noise = []
	for x, y in original_data:
		with_noise.append(y)
	for x, y in reduced_noise_data:
		without_noise.append(y)

	# If pyplt gets just a list, it fills up the x-values automatically.
	# I don't really care about the absolute x-position here, since I just 
	# need to compare the y-values at the same position.
	# just need a comparison so I 
	plt.plot(with_noise, 'ro', without_noise, 'bs')
	plt.plot(with_noise, 'ro')
	plt.plot(without_noise, 'bs')
	plt.title('RED = with noise, BLUE = withOUT noise')
	# Save the plot
	plt.savefig(save_file)
	# Clear the plot
	plt.clf()


# One function that does the stuff above.
# ATTENTION: THIS WORKS JUST FOR DATA THAT HAS AN AVAILABLE
# PREVIOUS MEASUREMENT AND A FILE WITH ANOTHER REFERENCE!
# 
# Hence this can NOT be used for the very first measurement!!!
def reduce_noise(infile, outfile, other_reference_measurement, previous_measurement, \
								plotfile, maximum_difference_between_points, \
									maximum_difference_for_interpolation):

	# First read the data from the actual analysis file to be checked for 
	# outliers.
	# This will be the file with the last measurement as reference.
	data = data_from_one_file(infile)
	# Second get the data from the same measurement but with the second to last 
	# measurement as reference and also the data from the previous analysis.
	# Both are needed to check if an outlier probably is real data and just looks
	# like an outlier if just the data for this one analysis is taken into account.
	extra_data = get_extra_data(other_reference_measurement, previous_measurement)
	other_reference_measurement_data = extra_data[0]
	previous_measurement_data = extra_data[1]

	# Third, find the flat sections in this data ...
	first_check = find_flat_sections(data, maximum_difference_between_points)
	# ... and sort into as real assumed data (the points in the flat sections) ...
	first_run_real_data = first_check[0]
	# ... and possible outliers (all points NOT in flat sections).
	first_run_outliers = first_check[1]

	# Fourth, figure out the start- and end-points of the non-flat sections
	# and what the slope of the line between these points is.
	undefined_sections = find_undefined_sections(first_run_real_data)

	# Fifth, check if points that were assumed to be outliers, because these were
	# not in a flat section, probably are on the line between the start- and
	# end-point between two flat sections (vulgo: in the gap).
	# The points can "jitter" a bit around the line in y-direction.
	# How big this "jittering" can be is determined by 
	# maximum_difference_for_interpolation.
	# 
	# I assume that "dynamic regions" (e.g steep strain gradients) probably 
	# also lead to higher "jitter" values without the value of the property of 
	# interest being an outlier. To take this into account, 
	# maximum_difference_for_interpolation is bigger then 
	# maximum_difference_between_points which determines how "flat" a flat
	# section needs to be.
	# 
	# So this check is more or less just checking if the y-value of a point
	# is not larger or smaller then point then a the value determined by the
	# expected line in the gap.
	# 
	# Formerly assumed outliers that turn out to be real data during this 
	# check are transferd from the outlier-dict to the real-data-dict.
	# and both (modified) dicts are returned by check_if_point_is_on_line() 
	# and become second_run_real_data and second_run_outliers below.
	second_check = check_if_point_is_on_line(undefined_sections, \
									first_run_real_data, first_run_outliers, \
										maximum_difference_for_interpolation)
	second_run_real_data = second_check[0]
	second_run_outliers = second_check[1]

	# Sixth, if outliers show up at the same position and with approx. the same
	# value in the measurement with the older reference, it is very likely that 
	# these are actually NOT outliers but real values. Here this will be 
	# checked to reduce the chance of falsely dismissing real data.
	# See also comment to check_other_data_for_peaks().
	# The dicts with the real data and the outliers will be changed accordingly
	# and returned by check_other_data_for_peaks().
	third_check = check_other_data_for_peaks(second_run_real_data, \
			second_run_outliers, other_reference_measurement_data, \
			previous_measurement_data, maximum_difference_between_points)
	third_run_real_data = third_check[0]
	third_run_outliers = third_check[1]

	# Seventh, when using the running reference method to obtain the strain 
	# data, I need to add up all the different measurements. This means that
	# if outliers are detected, these can not just be left out, but must be
	# substituted in a meaningful when. The substituted values will be all
	# on the the line between two flat sections.
	fourth_run_real_data = interpolate_values(undefined_sections, third_run_real_data)

	# Eigth, situations may occur in which I can not interpolate. In these cases
	# just fill in the original data.
	# See also comment to fill_up_the_data()
	final_real_data = fill_up_the_data(data, fourth_run_real_data)

	# Ninth, since a dict is unsorted, I need to sort the data before writing to
	# file.
	sorted_x_values = sorted(final_real_data.keys())
	data_to_be_written = []
	for x in sorted_x_values:
		data_to_be_written.append((x, final_real_data[x]))

	# Tenth, write the sorted list to the outfile.
	write_to_file(outfile, data_to_be_written)

	# Eleventh, due to "lucky" add-up of noise data some noise-points are 
	# considered to be real points. A test on approx. 50.000 data-points has 
	# show that this is the case for ca. 0,02 per cent of the points. So I 
	# have to check the data anyway manually to get rid of these points.
	# I make pictures of each data-set that shall help me with that.
	plot_data(data, data_to_be_written, plotfile)


# The very first file will NOT undergo the noise reducing algorithm.
# See the definitions and comments in the above functions why this is the case.
# Anyhow, of course this file is needed in the same format as the other noise 
# reduced files (JUST the table with the values) and with a proper filename 
# ("0001_noise_reduced.txt") in the correct folders.
# But the raw analyzed file has additional data above the table and the 
# filename is not correct. I take care of this here.
def make_correct_first_file(file_location, name_base):
	first_file_addendum = name_base + '0001_Lower.txt'
	first_infile = os.path.join(file_location, first_file_addendum)
	first_outfile = os.path.join(file_location, '0001_noise_reduced.txt')

	data = []
	# Don't add everything, JUST add sth. if it belongs to the table 
	# of values.
	add = False

	# Open the first file and extract the data.
	with open(first_infile, 'r') as f:
		for line in f:
			if 'Length' in line or 'microstrain' in line \
						or '(deg C)' in line or 'Spectral' in line:
				add = True
			elif add:
				data.append(line.strip())

	# Now write this to the correct outfile.
	with open(first_outfile, 'w') as f:
		for element in data:
			f.write('%s\n' % element)


# A function that does everything to move all the files into the
# correct folders.
# ATTENTION: It is assumed that these folders exist.
def move_files(file_location):
	# The raw analysis files are located in file_location.
	# However, this is just a subfolder and I need to move all the noise
	# reduced files into other folders that are "one level above" the
	# RAW-folder.
	folder_base = file_location.split('RAW')[0]

	# Under windows " \ " is used instead of " / " to separate subfolders
	# in the file location name. 
	# This gave me major headaches how to account for that since " \ " has a 
	# special meaning in strings under python.
	# However, doing it this way I managed to get things done.
	# Especially os.path.join() is very useful to get operative system 
	# dependend seperators correct.
	all_folders = os.listdir(folder_base)

	for folder_name in all_folders:
		if 'Noise_reduced_before_manual_correction' in folder_name:
			first_folder = os.path.join(folder_base, folder_name)
		elif 'Noise_reduced_after_manual_correction' in folder_name:
			second_folder = os.path.join(folder_base, folder_name)
		elif 'PNG' in folder_name:
			png_folder = os.path.join(folder_base, folder_name)

	all_sorted_filenames = sorted(os.listdir(file_location))

	for filename in all_sorted_filenames:
		if "_noise_reduced" in filename:
			from_here = os.path.join(file_location, filename)
			to_there_1 = os.path.join(first_folder, filename)
			to_there_2 = os.path.join(second_folder, filename)
			shutil.copy2(from_here, to_there_1)
			shutil.move(from_here, to_there_2)
		# Since I never know if the filename ending is upper or lower case when
		# these files are created, I just convert the filename to all caps.
		elif ".PNG" in filename.upper():
			from_here = os.path.join(file_location, filename)
			to_there = os.path.join(png_folder, filename)
			shutil.move(from_here, to_there)





## ## ## ## ## Here the execution of the program starts. ## ## ## ## ##



# That I can call main() from other modules with the correct parameters 
# I decided to have file_location, name_base and maximum_number as parameters.
# 
# To have this however also as a standalone program, I ask for these 
# parameters in the if-construct that is called when this program is called
# on the shell.
def main(file_location, name_base, maximum_number):
	maximum_difference_between_points = 300
	maximum_difference_for_interpolation = 500

	# ATTENTION: DON'T CHECK THE FIRST FILE!!! 
	# IT DOESN'T HAVE A PREVIOUS MEASUREMENT FILE AND THAT WILL CAUSE ERRORS!
	for i in range(2, (maximum_number + 1)):
		print "noise reducing file %s" % i
		# Just generate for each file the correct filenames to work with.
		# The above mentioned OBR measurement/analysis program sets
		# the word "_Lower" automatically at the end of each filename.
		# If OBR_analyzer_simple.py program was used to automatically press
		# the buttons for analysis, the file with the second to last measurement
		# as reference file automatically has "_older_reference_Lower" at the 
		# end of the filename.
		# 
		# # zfill() adds zeros in front of the given string. 
		# i is a number, not a string. Thus I use str() to convert it to a 
		# string first.
		# 
		# Thanks to windows I need to jump through hoops to get the 
		# proper filenames.
		infile_addendum = name_base + str(i).zfill(4) + '_Lower.txt'
		infile = os.path.join(file_location, infile_addendum)
		outfile_addendum = str(i).zfill(4) + '_noise_reduced.txt'
		outfile = os.path.join(file_location, outfile_addendum)
		other_reference_measurement_addendum = name_base + \
								str(i).zfill(4) + '_older_reference_Lower.txt'
		other_reference_measurement = os.path.join(file_location, \
											other_reference_measurement_addendum)
		previous_measurement_addendum = name_base + str((i-1)).zfill(4) + '_Lower.txt'
		previous_measurement = os.path.join(file_location, previous_measurement_addendum)
		plotfile_addendum = str(i).zfill(4) + '_plot.png'
		plotfile = os.path.join(file_location, plotfile_addendum)

		# Here the actual noise reduction takes place, see the 
		# function-definition(s) above.
		reduce_noise(infile, outfile, other_reference_measurement, \
									previous_measurement, plotfile, \
									maximum_difference_between_points, \
									maximum_difference_for_interpolation)

	# When all files are corrected for outliers, ...
	# 
	# ... make a proper first file 
	# (see comment to make_correct_first_file()) ...
	make_correct_first_file(file_location, name_base)
	# ... and move the files into the correct folders.
	move_files(file_location)





# When this program is called on the console, main() is executed.
# See comment to main() why I do here more then just executing main().
if __name__ == '__main__':
	print """
It is assumed that Filesorter.py was used to sort the raw analysis files.
That means that the following folders exist:
> Noise_reduced_after_manual_correction
> Noise_reduced_before_manual_correction
> PNG
> RAW
Also it is assumed that the raw analysis files are in the "RAW" folder.

If this is not the case all files will still be noise reduced, but can not be 
moved to the correct folders.\n
"""
	file_location = raw_input('Raw analysis files location (e.g. C:\OBR\Sample_23\RAW): ')
	name_base = raw_input('Analysis files name base (e.g. 2016-12-13_sample_23017_): ')
	maximum_number = int(raw_input('Highest file number: '))

	main(file_location, name_base, maximum_number)









